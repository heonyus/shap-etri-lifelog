SHAP는 이미 학습된 모델 $f$의 예측을 설명하는 방법.

$$ f(x)=ϕ_0​+∑^M_{i=1}​ϕ_i​ $$

- 모델의 한 sample $x$에 대한 예측값 $f(x)$
    - 기저값 $ϕ_0​$ 위에 각 특성(feature) $i$가 그 sample에서 기여한 양 $ϕ_i$ 들을 더해 만든 것
    - 즉, 기저값($ϕ_0​$) $+$ 특성별 변화량($ϕ_i$)들의 합 $=$ 최종 예측이라는 가산적 설명 모델(additive feature attribution model)

$$ ϕ_0​=E_{X∼P}[f(X)] $$

- 입력백터 $X$: 모델$f$에 들어가는 feature 묶음 (예: [steps, hr, screen_time,...])
- 배경분포 $P$: 입력 $X$가 나오는 배경분포(background distribution)
- $E_{X∼P}[f(X)]$: $P$에서 무작위로 $X$를 뽑아 모델 예측($f(x)$)를 계산하고 그 값을 평균
- 기저값 $ϕ_0​$: "아무 feature"도 쓰지 않고 평균적인 상황($P$)에서 모델 $f$가 내놓는 평균 예측

$$ \hatϕ_​0​=1/n∑^n_{​j=1}​f(x^{(j)}) $$
- $ ϕ_0​=E_{X∼P}[f(X)] $ 은 구하려는 기저값, 실전에서는 배경 표본 $B=[{x(1),…,x(n)}]$ 에서의 표본평균 ($ \hatϕ_​0$) 으로 근사사